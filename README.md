# Kapten

Kapten is a meta orchestration framework for building R and Python data pipelines with Prefect. It was built after experience with Prefect, tuning it from a general-purpose workflow tool to a data pipeline solution. It right-sizes tasks, tests changes, and streamlines development and operation.

Define your tasks and their dependencies in YAML and Kapten will render a pipeline runnable locally or on AWS with one click.

## Features

  - **Testable tasks** : Built-in change detection
  - **Right-sizing**: CPU and memory requirements per task
  - **Branch-based development**: Clone pipeline state across branches
  - **Update in-place**: Tweak and run a subset of the pipeline
  - **Less boilerplate**: Launch R scripts without writing any wrapper code
  - **Less vendor lock-in**: Any orchestration framework could be rendered, not just Prefect

## Basic Example

Here's a toy example of a pipeline called `example` with 3 tasks: `A`, `B`, `C`. We first define the graph of tasks, where `A` has no dependencies, `B` depends on `A`, and `C` depends on `A` and `B`.

```yaml
graphs:
  example:
    tasks:
      A:
      B: A
      C: [A, B]
```

In the same YAML file, we can define the tasks (2 R tasks, 1 Python task):

```yaml
tasks:
  A:
    r_script: "A/run.R"
    outputs: ["A.csv"]
  B:
    r_script: "B/run.R"
    outputs: ["B.csv"]
  C:
    py_script: true
    outputs: ["C.csv"]
```

The `r_script` and `py_script` fields define the location of the script to run. In the Python case, there must exist a script `tasks/C.py` with a function `C()`. The `outputs` field are the files generated by the script.

## Configuration

There are two main configuration files: pyproject.toml and tasks.yaml

### pyproject.toml

Your project is expected to use a pyproject.toml and contain a `tool.kapten` section.

<table>
  <thead>
  <tr><td>Field</td><td>Description</td><td>Required</td></tr>
  </thead>
  <tbody>
  <tr><td>flows-dir</td><td>The output directory where kapten will render your flow files to</td><td>Yes</td></tr>
  <tr><td>py-tasks-dir</td><td>The directory where any Python task code exists</td><td>Yes</td></tr>
  <tr><td>tasks-conf-path</td><td>The filepath to the tasks.yaml file defining your pipeline</td><td>Yes</td></tr>
  <tr><td>docker-image</td><td>The name of the Docker image to build and push to Prefect</td><td>Yes</td></tr>
  </tbody>
</table>

Example:

```toml
[tool.kapten]
flows-dir = "py_src/flows"
py-tasks-dir = "py_src/tasks"
tasks-conf-path = "py_src/tasks.yaml"
docker-image = "nibrs-estimation-pipeline:latest"
```

### tasks.yaml

Create a file, `tasks.yaml` that contains definitions of the graphs of tasks and the tasks themselves.

<table>
  <thead>
  <tr><td>Field</td><td>Value Description</td><td>Required</td></tr>
  </thead>
  <tbody>
  <tr><td>graphs</td><td>A dictionary of graph IDs and graph objects</td><td>Yes</td></tr>
  <tr><td>graphs.[id]</td><td>A dictionary representing a graph (nodes and edges)</td><td>Yes</td></tr>
  <tr><td>graphs.[id].tasks</td><td>An ordered dictionary of task IDs (nodes) and their dependencies (edges)</td><td>Yes</td></tr>
  <tr><td>graphs.[id].tasks.[task_id]</td><td>A list of dependency task IDs. If no dependencies, leave blank or use an empty list `[]`.</td><td>Yes</td></tr>
  <tr><td>tasks</td><td>A dictionary of task names and task objects</td><td>Yes</td></tr>
  <tr><td>tasks.[task_id]</td><td>A dictionary representing a task</td><td>Yes</td></tr>
  <tr><td>tasks.[task_id].r_script</td><td>A string with the filepath to the R script relative to the `r-tasks-dir`</td><td>No</td></tr>
  <tr><td>tasks.[task_id].prefix_args</td><td>A string that will be inserted before the `Rscript` command-line call</td><td>No</td></tr>
  <tr><td>tasks.[task_id].cli_args</td><td>A string that will be inserted at the end of the `Rscript` command-line call</td><td>No</td></tr>
  <tr><td>tasks.[task_id].py_script</td><td>If a string, the filepath to the Python script relative to the `py-tasks-dir`; if simply `true`, the filename is assumed to be the task_id.</td><td>No</td></tr>
  <tr><td>tasks.[task_id].cache_result</td><td>A boolean, if `true`, the Python script return value will be saved in the cache database, DynamoDB. If this value is a large list (e.g. 50k items), it will be sharded across DynamoDB items for scalability.</td><td>No</td></tr>
  <tr><td>tasks.[task_id].iterable_item</td><td>If `cache_result` is true and the result is a list, `iterable_item` is a string naming each item. The iterable item can also be a combination of values delimited by commas, e.g. US_STATE,ZIP_CODE</td><td>No</td></tr>
  <tr><td>tasks.[task_id].map_over</td><td>A string that corresponds to an `iterable_item` in a dependency. Setting `map_over` will call this task for each item in the dependency result list. If this task is a Python task, the `iterable_item` will be passed as a function argument. If this task is an R task, the `iterable_item` will be passed as an environment variable to the R script. If the `iterable_item` is a comma-delimited combo, the values will be split and passed in separately.</td><td>No</td></tr>
  <tr><td>tasks.[task_id].outputs</td><td>A list of files that the script outputs</td><td>No</td></tr>
  <tr><td>tasks.[task_id].aws_vars</td><td>A dictionary that can contain two fields, cpu and memory, corresponding to the <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html#task_size">Fargate task definition values</a></td><td>No</td></tr>
  <tr><td>tasks.[task_id].dask_worker</td><td>When the task is a mapped task (map_over is set), a dictionary that can contain two fields, cpu and memory, which will be used for each dask worker</td><td>No</td></tr>
  <tr><td>tasks.[task_id].tags</td><td>A list of strings, which will be used as the Prefect task's <a href="https://docs.prefect.io/v3/develop/write-tasks#tags">tags</a>; useful for <a href="https://docs.prefect.io/v3/develop/task-run-limits#limit-concurrent-task-runs-with-tags">concurrency limits</a></td><td>No</td></tr>
  </tbody>
</table>

Example:

```yaml
graphs:
  my_graph:
    tasks:
      A:
      B: [A]

tasks:
  A:
    py_script: true
    cache_result: true
    iterable_item: US_STATE
    aws_vars:
      cpu: 256
      memory: 512
  B:
    r_script: "B/run.R"
    map_over: US_STATE
    outputs:
    - "${US_STATE}.csv"
```

In this example, `my_graph` is the name of a graph (also known as a pipeline or a DAG). The graph object includes the `tasks` field, which is a dictionary of task names and a list of their corresponding dependencies (other tasks).

Task `A` will call the `A()` function in `A.py` and store the result in the cache database.

Task `B` will map over the result list, setting `US_STATE=${US_STATE}` as an environment variable for every call of the R script `B/run.R`

For example, if `A` returned the list `['NC', 'SC']`, `B/run.R` would be called 2 times, once with `US_STATE=NC` and once with `US_STATE=SC`













